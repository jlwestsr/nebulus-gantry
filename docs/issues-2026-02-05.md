# Gantry Issues & Feature Requests — 2026-02-05

**Source:** Manual walkthrough of running Gantry instance
**Status:** Captured, not yet triaged

---

## Admin

### 1. Users — No Edit Capability

- No way to edit existing users from the Admin > Users screen
- Need: edit display name, role, active status, reset password

### 2. Container Services — Empty

- Admin > Container Services shows nothing listed
- Expected: list of running Docker containers with status

### 3. Models — CRUD for LLM Host Models

- Admin > Models tab should support Create/Read/Update/Delete for models on the LLM host
- Ability to download new models directly from the UI
- Manage models served by TabbyAPI (or whatever backend is configured)

### 4. Logs — Disconnected

- Admin > Logs tab shows nothing, says "disconnected"

### 5. Logs — Service Selector Broken

- Changing the service in the select dropdown returns no logs for any service

---

## User Experience

### 6. Dashboard — Welcome Prompt on First Login

- Currently the dashboard (center area) is just a static message
- Should present a meaningful prompt or onboarding experience when user first logs in

### 7. Welcome Message — Personalized Greeting

- Include user's first name
- Time-of-day greeting: "Good morning", "Good afternoon", "Good evening"

### 8. Password Change — Missing

- Users have no way to change their own password
- Need a password change form in user settings/profile

---

## Chat

### 9. Model Identity & Switching **[RESOLVED — 2026-02-06]**

- **Bug:** Asked the AI which model it was, responded "I'm Nebulu, a helpful AI assistant" instead of reporting the actual loaded model (e.g. Qwen2.5-Coder-14B)
  - **Fixed:** System prompt now includes actual model name from TabbyAPI
- **Feature:** Need a way to see and switch models during a chat session
  - **Implemented:** Input box button approach (selected option)
  - Model selector dropdown in message input shows all available models
  - Active model marked with "loaded" badge
  - Auto-switching: When user selects a different model, backend automatically switches before streaming response
  - Loading indicator during model switch (5-30 seconds typical)
  - Graceful error handling if model switch fails

### 10. Chat Balloons — Timestamp & Token Details

- Hovering over chat balloons shows a timestamp — make these details more visible (not hover-only)
- Include token usage information (prompt tokens, completion tokens, total)
- Toggle visibility on/off in Options screen

### 11. AI Response Balloon — Performance Metrics

- Show token usage (prompt/completion/total) and generation speed (tokens/sec) on AI responses
- Toggle on/off in Options screen

---

## Settings

### 12. Options Screen — Needs Scoping

- Options screen does not yet exist as a fully scoped feature
- Should house toggles for:
  - Token usage display in chat
  - Generation speed display in chat
  - Timestamp visibility
  - Other user preferences TBD

### 13. Model Section — No Active Model

- Settings > Model section shows no active model listed
- Should display the currently loaded/selected model
